{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(r\"Inputs/dataset_alpha_betha.csv\")\n",
    "# Reemplazar valores ' '\n",
    "data.replace(' ', 0, inplace=True)\n",
    "\n",
    "# Preprocesamiento (preprocesamiento básico incluye el manejo de columnas numéricas y categóricas)\n",
    "# Identificar las columnas numéricas y categóricas\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Excluir 'autoID' y 'Class' que no son relevantes para la predicción\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['autoID','Demand', 'Class']]\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['autoID','Demand', 'Class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor\n",
      "MAE: 771.133796170923\n",
      "MSE: 1193464.7737204763\n",
      "R²: 0.7705939259313704\n",
      "\n",
      "Linear Regression\n",
      "MAE: 891.5398243435059\n",
      "MSE: 1269184.7630785352\n",
      "R²: 0.7560391390037262\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "MAE: 726.1376992628691\n",
      "MSE: 1033287.1443245846\n",
      "R²: 0.801383038373107\n",
      "\n",
      "XGB Regressor\n",
      "MAE: 762.1840422506447\n",
      "MSE: 1203593.2284850655\n",
      "R²: 0.7686470150947571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocesador y Pipeline para diferentes regresores\n",
    "\n",
    "# 1. RandomForestRegressor\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 2. LinearRegression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# 3. GradientBoostingRegressor\n",
    "pipeline_gb = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. XGBRegressor\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])),\n",
    "    ('regressor', XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Dividir el conjunto de datos\n",
    "X = data.drop(columns=['autoID', 'Demand', 'Class'])  # Features (sin la columna objetivo)\n",
    "y = data['Demand']  # Target variable (Demanda)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "\n",
    "# 1. RandomForestRegressor\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Regressor\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_rf)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "print(f\"R²: {r2}\\n\")\n",
    "\n",
    "# 2. LinearRegression\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred_lr)\n",
    "print(\"Linear Regression\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_lr)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "print(f\"R²: {r2}\\n\")\n",
    "\n",
    "# 3. GradientBoostingRegressor\n",
    "pipeline_gb.fit(X_train, y_train)\n",
    "y_pred_gb = pipeline_gb.predict(X_test)\n",
    "print(\"Gradient Boosting Regressor\")\n",
    "r2 = r2_score(y_test, y_pred_gb)\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_gb)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "print(f\"R²: {r2}\\n\")\n",
    "\n",
    "# 4. XGBRegressor\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "print(\"XGB Regressor\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_xgb)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_xgb)}\")\n",
    "print(f\"R²: {r2}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
